{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3707abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 18:47:03.292851: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-23 18:47:03.318009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-23 18:47:03.767848: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-12-23 18:47:04.565450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-23 18:47:04.582830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-23 18:47:04.584330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-12-23 18:47:04.585515: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2052] TensorFlow was not built with CUDA kernel binaries compatible with compute capability 12.0. CUDA kernels will be jit-compiled from PTX, which could take 30 minutes or longer.\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "from waymo_open_dataset.wdl_limited.camera.ops import py_camera_model_ops\n",
    "\n",
    "from waymo_open_dataset.protos import end_to_end_driving_data_pb2 as wod_e2ed_pb2\n",
    "from waymo_open_dataset.protos import end_to_end_driving_submission_pb2 as wod_e2ed_submission_pb2\n",
    "# Replace this path with your own tfrecords.\n",
    "# This tutorial is based on using data in the E2E Driving proto format directly,\n",
    "# so choose the correct dataset version.\n",
    "DATASET_FOLDER = '/home/hansung/end2end_ad/datasets/waymo_open_dataset_end_to_end_camera_v_1_0_0' #Raw data tfrecords directory. Modify\n",
    "OUTPUT_DIR = \"/home/hansung/OpenEMMA/waymo_dataset\" #Modify\n",
    "\n",
    "TRAIN_FILES = os.path.join(DATASET_FOLDER, \"training_*.tfrecord-*\")\n",
    "VALIDATION_FILES = os.path.join(DATASET_FOLDER,\"val_*.tfrecord-*\")\n",
    "TEST_FILES = os.path.join(DATASET_FOLDER, \"test_*.tfrecord-*\")\n",
    "dataset_mode = 'val' #['val','testing']\n",
    "if dataset_mode == 'val':\n",
    "    filenames = tf.io.matching_files(VALIDATION_FILES)\n",
    "else:\n",
    "    filenames = tf.io.matching_files(TEST_FILES)\n",
    "dataset = tf.data.TFRecordDataset(filenames, compression_type='')\n",
    "dataset_iter = dataset.as_numpy_iterator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3f9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "def yaw_to_rotmat_z(yaw: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"yaw: (T,) -> R: (T,3,3)\"\"\"\n",
    "    yaw = yaw.reshape(-1)\n",
    "    c = torch.cos(yaw)\n",
    "    s = torch.sin(yaw)\n",
    "    R = torch.zeros((yaw.numel(), 3, 3), dtype=torch.float32, device=yaw.device)\n",
    "    R[:, 0, 0] = c\n",
    "    R[:, 0, 1] = -s\n",
    "    R[:, 1, 0] = s\n",
    "    R[:, 1, 1] = c\n",
    "    R[:, 2, 2] = 1.0\n",
    "    return R\n",
    "\n",
    "def _states_to_world_xyz_and_yaw(states_obj: Any) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      p_world: (T,3) float32\n",
    "      yaw:     (T,) float32   (estimated if missing)\n",
    "    \"\"\"\n",
    "    xs = getattr(states_obj, \"pos_x\", None)\n",
    "    ys = getattr(states_obj, \"pos_y\", None)\n",
    "    zs = getattr(states_obj, \"pos_z\", None)\n",
    "\n",
    "    if xs is None or ys is None:\n",
    "        return torch.zeros((0, 3), dtype=torch.float32), torch.zeros((0,), dtype=torch.float32)\n",
    "\n",
    "    x = torch.as_tensor(list(xs), dtype=torch.float32)\n",
    "    y = torch.as_tensor(list(ys), dtype=torch.float32)\n",
    "    T = int(x.numel())\n",
    "    if T == 0:\n",
    "        return torch.zeros((0, 3), dtype=torch.float32), torch.zeros((0,), dtype=torch.float32)\n",
    "\n",
    "    if zs is None or len(zs) == 0:\n",
    "        z = torch.zeros((T,), dtype=torch.float32)\n",
    "    else:\n",
    "        z = torch.as_tensor(list(zs), dtype=torch.float32)\n",
    "        if z.numel() != T:\n",
    "            z = torch.zeros((T,), dtype=torch.float32)\n",
    "\n",
    "    p_world = torch.stack([x, y, z], dim=-1)  # (T,3)\n",
    "\n",
    "    # Prefer provided yaw/heading if it exists\n",
    "    yaw = None\n",
    "    if hasattr(states_obj, \"yaw\"):\n",
    "        yaw = torch.as_tensor(list(states_obj.yaw), dtype=torch.float32)\n",
    "    elif hasattr(states_obj, \"heading\"):\n",
    "        yaw = torch.as_tensor(list(states_obj.heading), dtype=torch.float32)\n",
    "\n",
    "    # Otherwise estimate yaw from dx,dy\n",
    "    if yaw is None or yaw.numel() != T:\n",
    "        dx = torch.diff(x, prepend=x[:1])\n",
    "        dy = torch.diff(y, prepend=y[:1])\n",
    "        speed2 = dx * dx + dy * dy\n",
    "        yaw = torch.atan2(dy, dx)\n",
    "        yaw = torch.where(speed2 < 1e-6, torch.zeros_like(yaw), yaw)\n",
    "        yaw[0] = yaw[1] if T > 1 else 0.0\n",
    "\n",
    "    return p_world, yaw\n",
    "\n",
    "def _world_to_local(\n",
    "    p_world: torch.Tensor,\n",
    "    yaw_world: torch.Tensor,\n",
    "    p0: torch.Tensor,\n",
    "    yaw0: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert world positions/orientations to local frame at (p0,yaw0).\n",
    "\n",
    "    Returns:\n",
    "      xyz_local: (T,3)\n",
    "      rot_local: (T,3,3) where rot_local[t] = R0^T * R_world[t]\n",
    "    \"\"\"\n",
    "    R_world = yaw_to_rotmat_z(yaw_world)  # (T,3,3)\n",
    "    R0 = yaw_to_rotmat_z(yaw0[None])[0]   # (3,3)\n",
    "    R0_inv = R0.t()\n",
    "\n",
    "    # xyz_local = R0^T (p - p0)\n",
    "    xyz_local = torch.einsum(\"ij,tj->ti\", R0_inv, (p_world - p0))\n",
    "\n",
    "    # rot_local = R0^T R_world\n",
    "    rot_local = torch.einsum(\"ij,tjk->tik\", R0_inv, R_world)\n",
    "    return xyz_local, rot_local\n",
    "\n",
    "INTENT = {\n",
    "    0: \"UNKNOWN\",\n",
    "    1: \"GO_STRAIGHT\",\n",
    "    2: \"GO_LEFT\",\n",
    "    3: \"GO_RIGHT\",\n",
    "}\n",
    "\n",
    "def load_waymo_e2e_data(\n",
    "    e2e_data: Any,\n",
    "    time_step: float = 0.25,\n",
    "    camera_features: Optional[List[Union[str, int]]] = None,\n",
    "    resize_mode: str = \"min_hw\",                 # \"min_hw\" or \"fixed\"\n",
    "    fixed_hw: Tuple[int, int] = (576, 1024),     # used if resize_mode==\"fixed\"\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    # --------------------\n",
    "    # States\n",
    "    # --------------------\n",
    "    past = e2e_data.past_states\n",
    "    future = e2e_data.future_states\n",
    "    e2e_frame = e2e_data.frame\n",
    "    # Build world tensors\n",
    "    p_past_w, yaw_past_w = _states_to_world_xyz_and_yaw(past)\n",
    "    p_fut_w,  yaw_fut_w  = _states_to_world_xyz_and_yaw(future)\n",
    "\n",
    "    Th = p_past_w.shape[0]\n",
    "    Tf = p_fut_w.shape[0]\n",
    "\n",
    "    # Choose t0 as last history state\n",
    "    if Th == 0:\n",
    "        raise RuntimeError(\"past_states is empty; cannot define t0.\")\n",
    "\n",
    "    p0 = p_past_w[-1]          # (3,)\n",
    "    yaw0 = yaw_past_w[-1]      # scalar tensor\n",
    "\n",
    "    # Localize both history and future w.r.t. SAME (p0,yaw0)\n",
    "    p_past_l, R_past_l = _world_to_local(p_past_w, yaw_past_w, p0, yaw0)\n",
    "    p_fut_l,  R_fut_l  = _world_to_local(p_fut_w,  yaw_fut_w,  p0, yaw0)\n",
    "\n",
    "    ego_history_xyz = np.stack([e2e_data.past_states.pos_x, e2e_data.past_states.pos_y, [0]*len(e2e_data.past_states.pos_x)], axis=1)\n",
    "    ego_history_rot = R_past_l.unsqueeze(0).unsqueeze(0)   # (1,1,Th,3,3)\n",
    "    ego_future_xyz  = np.stack([e2e_data.future_states.pos_x, e2e_data.future_states.pos_y, e2e_data.future_states.pos_z], axis=1)\n",
    "    ego_future_rot  = R_fut_l.unsqueeze(0).unsqueeze(0)    # (1,1,Tf,3,3)\n",
    "\n",
    "    # --------------------\n",
    "    # Timestamps (synthetic, relative to t0)\n",
    "    # (Waymo E2E frame doesn't always contain full per-step timestamps for states.)\n",
    "    # --------------------\n",
    "    t0_us = int(getattr(e2e_frame, \"timestamp_micros\", 0) or 0)\n",
    "    if t0_us == 0:\n",
    "        # fallback: use camera trigger time of first camera if present\n",
    "        try:\n",
    "            t0_us = int(e2e_frame.images[0].camera_trigger_time)\n",
    "        except Exception:\n",
    "            t0_us = 0\n",
    "\n",
    "    history_timestamps = (t0_us + np.arange(-(Th - 1), 1, 1) * int(time_step * 1e6)).astype(np.int64)\n",
    "    future_timestamps  = (t0_us + np.arange(1, Tf + 1, 1) * int(time_step * 1e6)).astype(np.int64)\n",
    "\n",
    "    # --------------------\n",
    "    # Images\n",
    "    # --------------------\n",
    "    # Decide which cameras to keep\n",
    "    # camera_features can be:\n",
    "    #   None -> keep all cameras in frame\n",
    "    #   ['ALL'] -> keep all\n",
    "    #   list of camera name strings or ints matching camera.name\n",
    "    imgs = list(e2e_frame.images)\n",
    "\n",
    "    if camera_features is not None and camera_features != [\"ALL\"]:\n",
    "        keep = set(camera_features)\n",
    "        imgs = [im for im in imgs if (im.name in keep) or (str(im.name) in keep)]\n",
    "\n",
    "    # Sort cameras deterministically by camera enum id\n",
    "    imgs = sorted(imgs, key=lambda im: int(im.name))\n",
    "\n",
    "    # Decode once; optionally compute min_hw\n",
    "    decoded = []\n",
    "    hws = []\n",
    "    for im in imgs:\n",
    "        arr = tf.io.decode_image(im.image, channels=3).numpy()  # (H,W,3) uint8\n",
    "        decoded.append((im, arr))\n",
    "        hws.append(arr.shape[:2])\n",
    "\n",
    "    if len(decoded) == 0:\n",
    "        raise RuntimeError(\"No decodable images in e2e_frame.images\")\n",
    "\n",
    "    if resize_mode == \"min_hw\":\n",
    "        target_h = min(h for h, w in hws)\n",
    "        target_w = min(w for h, w in hws)\n",
    "    elif resize_mode == \"fixed\":\n",
    "        target_h, target_w = fixed_hw\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown resize_mode: {resize_mode}\")\n",
    "\n",
    "    image_frames_list = []\n",
    "    camera_indices_list = []\n",
    "    timestamps_list = []\n",
    "\n",
    "    for im, arr in decoded:\n",
    "        h, w = arr.shape[:2]\n",
    "        if (h != target_h) or (w != target_w):\n",
    "            arr = cv2.resize(arr, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # (num_frames=1, 3, H, W)\n",
    "        frames_tensor = torch.from_numpy(arr).unsqueeze(0)  # (1,H,W,3)\n",
    "        frames_tensor = rearrange(frames_tensor, \"t h w c -> t c h w\").contiguous()\n",
    "\n",
    "        cam_idx = int(im.name)\n",
    "        cam_ts_us = int(getattr(im, \"camera_trigger_time\", t0_us) or t0_us)\n",
    "\n",
    "        image_frames_list.append(frames_tensor)\n",
    "        camera_indices_list.append(cam_idx)\n",
    "        timestamps_list.append(torch.tensor([cam_ts_us], dtype=torch.int64))\n",
    "\n",
    "    image_frames = torch.stack(image_frames_list, dim=0)               # (Ncam, 1, 3, H, W)\n",
    "    camera_indices = torch.tensor(camera_indices_list, dtype=torch.int64)\n",
    "    absolute_timestamps = torch.stack(timestamps_list, dim=0)          # (Ncam, 1)\n",
    "    camera_tmin = absolute_timestamps.min()\n",
    "    relative_timestamps = (absolute_timestamps - camera_tmin).float() * 1e-6\n",
    "\n",
    "    # Camera name map (Waymo)\n",
    "    camera_ind_name = {\n",
    "        1: \"FRONT\",\n",
    "        2: \"FRONT_LEFT\",\n",
    "        3: \"FRONT_RIGHT\",\n",
    "        4: \"SIDE_LEFT\",\n",
    "        5: \"SIDE_RIGHT\",\n",
    "        6: \"REAR\",\n",
    "        7: \"REAR_LEFT\",\n",
    "        8: \"REAR_RIGHT\",\n",
    "    }\n",
    "\n",
    "    camera_calibrations = e2e_data.frame.context.camera_calibrations\n",
    "    camera_intrinsics = []\n",
    "    camera_extrinsics = []\n",
    "    for cam_calibration in camera_calibrations:\n",
    "        intr = np.array(cam_calibration.intrinsic,dtype=np.float64)\n",
    "\n",
    "        fu, fv, cu, cv = intr[:4]\n",
    "        K = np.array([\n",
    "            [fu, 0., cu],\n",
    "            [0., fv, cv],\n",
    "            [0., 0., 1.]\n",
    "        ],dtype=np.float64)\n",
    "\n",
    "        dist = np.array([intr[4], intr[5], intr[6], intr[7], intr[8]], dtype=np.float64)\n",
    "        camera_intrinsics.append((K,dist))\n",
    "        T = np.array(cam_calibration.extrinsic.transform).reshape(4,4)\n",
    "        camera_extrinsics.append(T)\n",
    "\n",
    "    clip_id = getattr(getattr(e2e_frame, \"context\", None), \"name\", \"\")\n",
    "    ego_intent = INTENT[e2e_data.intent]\n",
    "    vehicle_pose = np.array(e2e_frame.images[0].pose.transform).reshape(4, 4)\n",
    "    \n",
    "\n",
    "    speed = np.sqrt((e2e_frame.images[0].velocity.v_x)**2 + (e2e_frame.images[0].velocity.v_y)**2 + (e2e_frame.images[0].velocity.v_z)**2)\n",
    "\n",
    "    return {\n",
    "        \"image_frames\": image_frames,                   # (Ncam,1,3,H,W) uint8\n",
    "        \"ego_history_xyz\": ego_history_xyz,             # (1,1,Th,3) local\n",
    "        \"ego_future_xyz\": ego_future_xyz,               # (1,1,Tf,3) local\n",
    "        \"ego_intent\": ego_intent,\n",
    "        \"camera_intrinsic\": camera_intrinsics,\n",
    "        \"camera_extrinsic\": camera_extrinsics,\n",
    "        \"vehicle_pose\": vehicle_pose,\n",
    "        \"speed\": speed\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5c28fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "106360it [44:40, 39.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#save data_dict\n",
    "import pickle\n",
    "import gzip\n",
    "import tqdm\n",
    "def save_dict_to_pickle(d: dict, path: str) -> None:\n",
    "    # Use highest protocol for speed/size; write in binary mode\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(d, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "save_path = OUTPUT_DIR + '/' + dataset_mode + '/'\n",
    "ctr = 0\n",
    "threshold = 0 #Since the dataset is large, you can choose which index you want to threshold when preprocessing data\n",
    "for bytes in tqdm.tqdm(dataset_iter):\n",
    "    \n",
    "    data = wod_e2ed_pb2.E2EDFrame()\n",
    "    data.ParseFromString(bytes)\n",
    "\n",
    "    i = int(data.frame.context.name.split('-')[1])\n",
    "    if i >= threshold:\n",
    "        data_dict = load_waymo_e2e_data(data,0.25,camera_features=['ALL'])\n",
    "        filename = data.frame.context.name + '.pkl'\n",
    "        save_dict_to_pickle(data_dict, save_path+filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2e",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
